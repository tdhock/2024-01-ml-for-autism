Title: A tutorial on interpretable machine learning
algorithms for understanding factors related to
childhood autism

Abstract: machine learning is a research area in computer science
which is concerned with algorithms which learn from large data
sets. For example, the National Survey of Children's Health (NSCH) is
a survey that results in a large data set that can be used with
machine learning -- can we predict if the child has autism, based on
the other survey responses? How accurately can we predict? And what
other survey responses are most useful for prediction? In this
tutorial, I will show how machine learning can be used to answer these
questions.

Slides: https://github.com/tdhock/2024-01-ml-for-autism/blob/main/HOCKING-slides-2024-02-26-ml-for-autism.pdf

See also code https://github.com/vas235/ASG3-machine-learning-prep from Vince which treats more than two years, and standardizes some variables between the years, using a JSON config file.

** TODOs

- subset on autism, try to predict autism service variables in that subset
  - H1: Receipt of AAP-recommended developmental screening  -- DevScrnng
  - H2. Early ASD Diagnosis at Age < 3 Years â€“ K2Q35A_1_Years
  - H3a. Current ASD Service Use including Medication -- AUTISMMED
  - H3b. Current ASD Service Use including Behavior Therapy -- AUTISMTREAT
  - H4: Optimal Access to ASD Services including Early Diagnosis (diagnosis age < 3 years) & Current Behavior Therapy Use for ASD (Might add early intervention here too because captured in the NSCH) -- K2Q35A_1_Years < 3 & AUTISMTREAT
- SOAK on years, also on periods (2016-2017=old survey questions,
  2018-2019=new questions before covid, 2020-2022=new questions after
  covid)

to launch the cross-validation experiments in parallel, adapt
[[file:download-nsch-mlr3batchmark.R]] and
- when constructing task.list, keep for loop over
  feature.names/model.name as in current code, if you want to check if
  certain groups of features are required for optimal prediction
  accuracy.
- when constructing learner.list, only keep glmnet/featureless if you
  want speed, otherwise keep rpart/xgboost/knn/etc if you want to
  compare with non-linear learning algorithms (potentially more
  accurate than the glmnet linear model).
- when constructing task.list, add a for loop over different
  target/output variables to predict (after subsetting on patients
  with Autism). Current code only considers one output "Autism" column
  (for all patients).
- when constructing task.list, add a for loop over different subset
  column: year or period (2016-2017=old survey questions,
  2018-2019=new questions before covid, 2020-2022=new questions after
  covid).
- all of the different algorithms, tasks, and train/test splits can be
  run in parallel, on NAU Monsoon, using mlr3batchmark package, see [[https://tdhock.github.io/blog/2024/hyper-parameter-tuning/][my
  blog]] and the references therein.

** 10 Oct 2024

[[file:2024-10-09-vince-data/clean-data.R]] reads data from github and verifies if everything is ok for ML analysis.

Still a few issues.

Two variables with 0 responses in 2016-2017

*** interest

k6q71_r Show Interest and Curiosity     5       7       5       2018,2019,2020,2021,2022        21175   Usually

=> no responses for "Usually" in 2016 nor 2017 -- added in 2018

other responses are: Sometimes, Never, Always, missing

so could be converted to "frequency of interest and curiosity" numeric variable: 0=Never, 1=Sometimes, 2=Usually, 3=Always (bigger values for more frequent)

or Vince suggests: Usually could just be collapsed back into Sometimes, in years 2018-2022 to make it work with 2016

[[file:2024-10-09-vince-data/heat-map-interest-values.png]]

The right heat map shows your proposed substitution (Usually changed to Sometimes), there still seems to be a significant change in proportions between 2017 and 2018.
I guess this can be attributed to the change in wording of the question.
for ML this is probably sub-optimal but it is probably better than the original variable, with Usually in some years and not others.


*** family

family  Family Structure        6       7       6       2017,2018,2019,2020,2021,2022   9950    Single father

family  Family Structure        6       7       6       2017,2018,2019,2020,2021,2022   6560    Grandparent household

=> neither of these two responses in 2016 -- added in 2017

other values are:

missing

Two biogical/adoptive parents, currently married

Other relation

Two parents (at least one not biological/adoptive), currently married

Single mother

Two biogical/adoptive parents, not currently married

Two parents (at least one not biological/adoptive), not currently married

so "family" really is categorical.

a way to work around this would be to convert "Single father" and "Grandparent household" to the response which they probably used for this in 2016: "Other relation"

[[file:2024-10-09-vince-data/heat-map-family-values.png]]

The left heat map shows the current data (variable: family) has a decrease in Other proportion from 6.8% in 2016 to 1.0 in 2017.
The right heat map shows the preoposed work around (variable: familyOld) has more stable Other proportion, 7.2% in 2017, and no entries for "Single Father" nor "Grandparent household"
So this figure suggests that the proposed work around should make this variable more comparable between years. (although we lose some granularity -- we could see differences between Single Father and Grandparent if we exclude 2016 from the analysis and keep the original family variable)

*** missing values

Three variables with no missing values in 2017 and after, but 2-3%
missing values in 2016.

#+begin_src 
The only thing I saw was the same one you mentioned:
a1_grade        Adult 1 - Highest Completed Year of School      1       10      factor  3.04    0.00
higrade_tvis    Highest Level of Education among Reported Adults, Detail        1       5       factor  2.38    0.00    0.00
higrade Highest Level of Education among Reported Adults        1       4       factor  2.38    0.00    0.00    0.00
=> no missing values for this variable on 2017+later, but 2-3% missing values in 2016, seems unusual to have zero vs not.
#+end_src


** 2 Oct 2024

[[file:2024-08-06-vince-data/clean-data.R]] reads [[file:2024-08-06-vince-data/clean-data/clean-data.rds]] and outputs

- columns with values present in only some years (probably need to standardize across years) [[file:2024-08-06-vince-data/clean-data-values-only-in-some-years.csv]]
- all column desc, TODO manually add categories [[file:2024-08-06-vince-data/clean-data-var-all-desc.csv]]
- columns with not only one desc value, TODO pick which one we should use for display [[file:2024-08-06-vince-data/clean-data-var-not-one-desc.csv]]
- values with diferent codes in different years, actually irrelevant for clean-data.rds because the values in that file are already decoded [[file:2024-08-06-vince-data/define_anomalies.csv]]

** 26 Mars 2024
[[file:figures-same-other/]] contains [[file:figures-same-other/NSCH_autism_error.csv][CSV]] and figures to show that it is not just size that matters.

[[file:figures-same-other/NSCH_autism_error_mean_sd_more.png]]

** 26 Feb 2024

[[file:HOCKING-slides-2024-02-26-ml-for-autism.tex]] makes
[[file:HOCKING-slides-2024-02-26-ml-for-autism.pdf]] slides
with new drawings

[[file:drawing-cv-feature-sets.svg]] makes
[[file:drawing-cv-feature-sets.pdf]]

[[file:drawing-cv-same-other-years.svg]] makes
[[file:drawing-cv-same-other-years-1.pdf]]
[[file:drawing-cv-same-other-years-2.pdf]]
[[file:drawing-cv-same-other-years-3.pdf]]
[[file:drawing-cv-same-other-years-4.pdf]]

** 23 Feb 2024

[[file:download-nsch-mlr3batchmark.R]] launches jobs, here is a preliminary analysis of how much time and memory they take:

#+BEGIN_SRC R
> usage.wide[order(megabytes_max), .(learner_id, task_id, megabytes_min, megabytes_median, megabytes_max, megabytes_length)]
                   learner_id        task_id megabytes_min megabytes_median megabytes_max megabytes_length
                       <char>         <char>         <num>            <num>         <num>            <int>
 1:         classif.cv_glmnet    behavior.15        0.0000           0.0000        0.0000               60
 2:         classif.cv_glmnet comorbidity.30        0.0000           0.0000        0.0000               60
 3:         classif.cv_glmnet     culture.14        0.0000           0.0000        0.0000               60
 4:       classif.featureless comorbidity.30        0.0000           0.0000        0.0000               60
 5:       classif.featureless  healthcare.88        0.0000           0.0000        0.0000               60
 6:             classif.rpart       birth.24        0.0000           0.0000        0.0000               60
 7:             classif.rpart comorbidity.30        0.0000           0.0000        0.0000               60
 8:             classif.rpart     culture.14        0.0000           0.0000        0.0000               60
 9:             classif.rpart  healthcare.88        0.0000           0.0000        0.0000               60
10:       classif.featureless     culture.14        0.0000           0.0000      184.3555               60
11:       classif.featureless       birth.24        0.0000           0.0000      185.0703               60
12:             classif.rpart    behavior.15        0.0000           0.0000      195.0234               60
13:       classif.featureless    behavior.15        0.0000           0.0000      196.5000               60
14:         classif.cv_glmnet       birth.24        0.0000           0.0000      419.1250               60
15:           classif.xgboost     culture.14      410.0664         425.7168      516.3867               60
16:           classif.xgboost       birth.24      411.4688         446.2695      518.8477               60
17:           classif.xgboost    behavior.15      413.1992         431.9512      519.3633               60
18:           classif.xgboost comorbidity.30      411.9727         451.4375      520.8359               60
19: classif.nearest_neighbors     culture.14      405.4688         465.7988      531.1367               60
20: classif.nearest_neighbors    behavior.15      401.6992         462.6016      552.0781               60
21: classif.nearest_neighbors       birth.24      409.3086         472.2266      588.5117               60
22: classif.nearest_neighbors comorbidity.30      435.0664         480.6035      594.1562               60
23:         classif.cv_glmnet  healthcare.88        0.0000         453.3457      606.5117               60
24:           classif.xgboost  healthcare.88      519.7617         614.1836      747.3711               60
25: classif.nearest_neighbors  healthcare.88      536.2422         613.3730      843.5859               60
26:            classif.ranger  healthcare.88     1192.5625        1192.5625     1192.5625                1
27:            classif.ranger comorbidity.30     1201.4414        1347.5469     1944.3164               30
28:            classif.ranger     culture.14      898.6367        1336.7637     1966.7070               60
29:            classif.ranger    behavior.15     1003.0703        1372.0977     2167.9062               60
30:            classif.ranger       birth.24     1244.2656        1758.0156     2780.9922               43
                   learner_id        task_id megabytes_min megabytes_median megabytes_max megabytes_length
> usage.wide[order(hours_max), .(learner_id, task_id, hours_min, hours_median, hours_max, hours_length)]
                   learner_id        task_id    hours_min hours_median    hours_max hours_length
                       <char>         <char>        <num>        <num>        <num>        <int>
 1:       classif.featureless     culture.14 0.0005555556 0.0008333333  0.001111111           60
 2:             classif.rpart     culture.14 0.0005555556 0.0008333333  0.001111111           60
 3:       classif.featureless    behavior.15 0.0005555556 0.0011111111  0.001388889           60
 4:       classif.featureless       birth.24 0.0005555556 0.0008333333  0.001388889           60
 5:             classif.rpart comorbidity.30 0.0008333333 0.0008333333  0.001388889           60
 6:             classif.rpart    behavior.15 0.0008333333 0.0011111111  0.001666667           60
 7:             classif.rpart       birth.24 0.0005555556 0.0008333333  0.001666667           60
 8:       classif.featureless comorbidity.30 0.0005555556 0.0011111111  0.001944444           60
 9:       classif.featureless  healthcare.88 0.0005555556 0.0009722222  0.001944444           60
10:             classif.rpart  healthcare.88 0.0008333333 0.0011111111  0.002222222           60
11:         classif.cv_glmnet     culture.14 0.0011111111 0.0016666667  0.002500000           60
12:         classif.cv_glmnet    behavior.15 0.0019444444 0.0025000000  0.003333333           60
13:         classif.cv_glmnet       birth.24 0.0013888889 0.0019444444  0.004722222           60
14:         classif.cv_glmnet comorbidity.30 0.0016666667 0.0027777778  0.005000000           60
15:         classif.cv_glmnet  healthcare.88 0.0047222222 0.0094444444  0.020000000           60
16:           classif.xgboost     culture.14 0.0102777778 0.0166666667  0.027777778           60
17:           classif.xgboost    behavior.15 0.0169444444 0.0254166667  0.048888889           60
18:           classif.xgboost comorbidity.30 0.0252777778 0.0477777778  0.080833333           60
19: classif.nearest_neighbors    behavior.15 0.0138888889 0.0291666667  0.084722222           60
20:           classif.xgboost       birth.24 0.0241666667 0.0366666667  0.087222222           60
21: classif.nearest_neighbors     culture.14 0.0122222222 0.0268055556  0.096666667           60
22: classif.nearest_neighbors       birth.24 0.0150000000 0.0306944444  0.099444444           60
23: classif.nearest_neighbors comorbidity.30 0.0183333333 0.0398611111  0.170277778           60
24:           classif.xgboost  healthcare.88 0.0608333333 0.1200000000  0.213333333           60
25: classif.nearest_neighbors  healthcare.88 0.0566666667 0.1898611111  0.798888889           60
26:            classif.ranger  healthcare.88 5.3941666667 5.3941666667  5.394166667            1
27:            classif.ranger     culture.14 1.1869444444 2.5109722222  6.713055556           60
28:            classif.ranger    behavior.15 1.5277777778 3.2013888889  8.618611111           60
29:            classif.ranger comorbidity.30 3.6255555556 4.6951388889 10.774444444           30
30:            classif.ranger       birth.24 2.4188888889 5.0616666667 12.538888889           43
                   learner_id        task_id    hours_min hours_median    hours_max hours_length
#+END_SRC

Looks like ranger is by far the slowest and more memory intensive, so
for now I will omit that.

Below we see that total time for CV experiment with 2700 iterations is
240 hours, so since we did this in a 4 hour time limit, this is about
60x speedup.

#+BEGIN_SRC R
2700: 3.194722222  1810.023 classif.nearest_neighbors     all.364
> sum(usage.long$hours)
[1] 240.7103
> sum(usage.long$hours)/4
[1] 60.17757
#+END_SRC

** 22 Feb 2024

[[file:download-nsch-convert-do.R]] makes [[file:download-nsch-convert-do-2019-2020.csv]]

#+BEGIN_SRC R
> out.dt[, table(survey_year, Autism)]
           Autism
survey_year   Yes    No
       2019   859 28003
       2020  1255 40826
#+END_SRC

[[file:download-nsch-counts.R]] separated out from [[file:download-nsch.R]]

** 18 Dec 2023

[[https://docs.google.com/spreadsheets/d/19Tm75T4wNN4yITlXuUMNVc22yzHmmzVcMY1GBVGsEnQ/edit#gid=0]]
is the source file for [[file:NSCH_categories.csv]]

[[file:download-nsch.R]] makes [[file:download-nsch-nrow-ncol.csv]] and [[file:download-nsch-column-counts.csv]]
and [[file:NSCH_categories_NA_counts.csv]] after which I manually added different categories for the least missing columns,
[[file:NSCH_categories_NA_counts_TDH.csv]]
